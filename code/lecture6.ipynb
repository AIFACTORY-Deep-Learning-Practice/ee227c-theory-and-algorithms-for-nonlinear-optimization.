{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 6: Accelerating gradient descent with Chebyshev polynomials\n",
    "\n",
    "[EE227C course page](https://ee227c.github.io/)  \n",
    "[Download ipynb file](https://ee227c.github.io/code/lecture6.ipynb)\n",
    "\n",
    "In this lecture we will derive an intriguing way to speed up gradient descent using Chebyshev polynomials. In doing so, we'll also fill in the details for a blog post I wrote four and a half years ago called [zen of gradient descent](http://blog.mrtz.org/2013/09/07/the-zen-of-gradient-descent.html), just in case you were still waiting on that.\n",
    "\n",
    "Overview:\n",
    "\n",
    "* [A closer look at quadratics](#quadratics)\n",
    "* [Connection to polynomial approximation](#polynomials)\n",
    "* [Chebyshev polynomials](#chebyshev)\n",
    "* [Accelerated gradient descent for quadratics](#acceleration)\n",
    "\n",
    "Bibliographic note: *Linear Algebra and its Applications* by Peter D. Lax has a fantastic exposition of this material in Chapter 17. It's generally a fabulous linear algebra text that I highly recommend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<link href=\"https://fonts.googleapis.com/css?family=Inconsolata:700\" rel=\"stylesheet\">\n",
       "<link href=\"https://fonts.googleapis.com/css?family=Bree+Serif\" rel=\"stylesheet\">\n",
       "\n",
       "<style>\n",
       "div.text_cell_render p, \n",
       "div.text_cell_render li, \n",
       "div.text_cell_render ul \n",
       "{ \n",
       "    font-size:20pt;\n",
       "    line-height:28pt; \n",
       "    font-family: 'Bree Serif', monospace;\n",
       "    text-align: justify;\n",
       "}\n",
       ".CodeMirror pre, .inner_cell pre, pre {\n",
       "    line-height:26pt;\n",
       "    font-size:20pt; \n",
       "    font-family: 'Inconsolata', monospace;\n",
       "}\n",
       "div.output_subarea {font-size:18pt;}\n",
       "div.text_cell_render h1 {\n",
       "    background-color:#ebf5fb;\n",
       "    line-height: 48pt;\n",
       "    font-size: 36pt;\n",
       "    color:#222;\n",
       "    text-align:center;\n",
       "}\n",
       "div.text_cell_render h2 {\n",
       "    background-color:#ebf5fb;\n",
       "    font-size: 30pt;\n",
       "    line-height: 48pt;\n",
       "    text-align:center;\n",
       "}\n",
       "div.text_cell_render h3 {\n",
       "    background-color:#ebf5fb;\n",
       "    font-size: 24pt;\n",
       "}\n",
       "div.text_cell_render h4 {\n",
       "    background-color:#ebf5fb;\n",
       "    font-size: 22pt;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from plotters import setup_layout\n",
    "setup_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"nuclear-norm\"></a>\n",
    "\n",
    "## Quadratics\n",
    "\n",
    "Consider minimizing the objective function\n",
    "\n",
    "<p>\n",
    "$$f(x) = \\frac12 x^\\top A x - b^\\top x$$\n",
    "</p>\n",
    "\n",
    "over all of $\\mathbb{R}^n,$ where $A\\in\\mathbb{R}^{n\\times n}$ is a symmetric positive definite matrix. We might for instance want to do this when we solve linear equations corresponding to a Laplacian operator. Note that\n",
    "\n",
    "<p>\n",
    "$$\\nabla f(x) = Ax-b\\qquad\\text{and}\\qquad \\nabla^2 f(x) = A.$$\n",
    "</p>\n",
    "\n",
    "As we can see, the gradient vanishes when $Ax=b.$\n",
    "We denote the condition number of the matrix by $\\kappa=\\beta/\\alpha$ where $\\beta=\\lambda_1(A)$ and $\\alpha=\\lambda_n(A)>0.$ In particular, we know that the objective function is $\\beta$-smooth and $\\alpha$-strongly convex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def quadratic(A, b, x):\n",
    "    \"\"\"Quadratic defined by A and b at x.\"\"\"\n",
    "    return 0.5 * x.dot(A.dot(x)) - b.dot(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already saw in Lecture 3 that gradient descent achieves a linear convergence rate of the form $\\exp(-t\\alpha/\\beta)$ for all $\\alpha$-strongly convex and $\\beta$-smooth functions. Let's rederive this result for quadratics, where it's almost trivial.\n",
    "\n",
    "Let $x^*$ be the unique solution of the linear system $Ax=b$ and put\n",
    "\n",
    "<p>\n",
    "$$\n",
    "e_t = \\|x_t-x^*\\|\n",
    "$$\n",
    "</p>\n",
    "\n",
    "where $x_{t+1}=x_t - \\eta_t (Ax_t-b)$ is defined recursively starting from some $x_0,$ and $\\eta_t$ is a step size we'll determine shortly.\n",
    "\n",
    "Note that $x^*$ satisfies (for any $t$)\n",
    "<p>\n",
    "$$\n",
    "x^* = (I-\\eta_t A)x^* +\\eta b.\n",
    "$$\n",
    "</p>\n",
    "\n",
    "Hence,\n",
    "<p>\n",
    "$$\n",
    "\\begin{align}\n",
    "e_{t+1} & = x_{t+1}-x^* \\\\\n",
    " & = (I-\\eta_t A)x_t +\\eta_t b - ( (I-\\eta_t A)x^* +\\eta_t b ) \\\\\n",
    " & = (I-\\eta_t A)e_t.\n",
    "\\end{align}\n",
    "$$\n",
    "</p>\n",
    "\n",
    "This gives us $e_t = p_t(A)e_0,$ where $p_t$ is the polynomial\n",
    "<p>\n",
    "$$\n",
    "p_t(a) = \\prod_{i=1}^t (1-\\eta_ta)\\,.\\qquad(*)\n",
    "$$\n",
    "</p>\n",
    "\n",
    "We can upper bound the norm of the error term as\n",
    "\n",
    "$$\n",
    "\\|e_t\\|\\le \\|p_t(A)\\|\\cdot\\|e_0\\|\\,.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since $A$ is a symmetric matrix with eigenvalues in $[\\alpha,\\beta],$ it's not hard to justify that\n",
    "\n",
    "<p>\n",
    "$$\n",
    "\\|p_t(A)\\|\\le \\max_{\\alpha\\le a\\le\\beta} \\left|p_t(a)\\right|\\,.\n",
    "$$\n",
    "</p>\n",
    "\n",
    "This leads to an intriguing problem: Among all polynomials that satisfy $p_t(0)=1$ we're looking for a polynomial whose magnitude is as small as possible in the interval $[\\alpha,\\beta].$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"polynomials\"></a>\n",
    "\n",
    "## The naive polynomial solution\n",
    "\n",
    "A naive solution is to choose a uniform step size $\\eta_t=2/(\\alpha+\\beta)$ in the expression $(*).$ This rescales the eigenvalues of $A$ just in the right way so that a simple calculation gives the bound:\n",
    "\n",
    "<p>\n",
    "$$\n",
    "\\|e_t\\|\\le \\left(1-\\frac1{\\kappa}\\right)^t\\|e_1\\|\n",
    "\\le \\exp\\left(-\\alpha t/\\beta\\right)\\|x_0-x^*\\|\\,.\n",
    "$$\n",
    "</p>\n",
    "\n",
    "This is exactly the rate we proved in Lecture 3 for any smooth and strongly convex function. So, no surprise yet.\n",
    "\n",
    "Let's look at this polynomial a bit closer. In the example below we choose $\\alpha=1$ and $\\beta=10$ so that $\\kappa=10.$ The relevant interval is therefore $[1,10].$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'kwargs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-88621c9b1289>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m14\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Naive polynomial'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'g-'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'degree 3'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'g--'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'max value'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'b-'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'degree 6'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'kwargs' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA1wAAAH8CAYAAADFZq4DAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f916eb0a510>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def p(k, x, alpha=1.0, beta=10.0):\n",
    "    return np.power(1.0 - 2.0*x/(alpha+beta), k)\n",
    "\n",
    "alpha, beta = 1.0, 10.0\n",
    "xs = np.linspace(0, beta, 100)\n",
    "plt.figure(figsize=(14, 8))\n",
    "plt.title('Naive polynomial')\n",
    "plt.plot(xs, p(3, xs), 'g-', label='degree 3', **kwargs)\n",
    "plt.plot(xs, [p(3, alpha)]*len(xs), 'g--', label='max value')\n",
    "plt.plot(xs, p(6, xs), 'b-', label='degree 6', **kwargs)\n",
    "plt.plot(xs, [p(6, alpha)]*len(xs), 'b--', label='max value')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doubling the degree roughly halves the maximum absolute value that the polynomial attains in the interval $[\\alpha, \\beta].$\n",
    "\n",
    "Can we do better than this? Surprisingly, the answer is yes!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"chebyshev\"></a>\n",
    "\n",
    "## Chebyshev polynomials\n",
    "\n",
    "Chebyshev polynomials turn out to give an optimal answer to the question that we asked. Suitably rescaled, they minimize the absolute value in a desired interval $[\\alpha, \\beta]$ while satisfying the normalization constraint of having value $1$ at the origin.\n",
    "\n",
    "Before we do the rescaling, we will first look at the standard Chebyshev polynomials (\"of the first kind\") on the interval $[-1,1].$ These polynomials have several natural equivalent definitions. We define the Chebyshev polynomial $T_k$ recursively as follows:\n",
    "\n",
    "<p>\n",
    "$$\n",
    "\\begin{align}\n",
    "T_0(a) &= 0\\\\\n",
    "T_1(a) &= x\\\\\n",
    "T_k(a) &=2aT_{k-1}(a)-T_{k-2}(a),\\qquad k\\ge 2.\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def T(k, a):\n",
    "    \"\"\"Chebyshev polynomial of degree k\"\"\"\n",
    "    if k <= 1:\n",
    "        return a**k\n",
    "    else:\n",
    "        return 2.0*a*T(k-1, a) - T(k-2, a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the first 5 Chebyshev polynomials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xs = np.linspace(-1, 1, 100)\n",
    "plt.figure(figsize=(14, 10))\n",
    "_ = [plt.plot(xs, T(k, xs), **kwargs) for k in range(0, 5)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far so good. The magic happens when we rescale these polynomials so as to satisfy our requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rescaled Chebyshev polynomials\n",
    "\n",
    "Recall that the eigenvalues of the matrix we consider are in the interval $[\\alpha, \\beta].$ We need to rescale the Chebyshev polynomials so that they're supported on this interval and still attain value $1$ at the origin. This is accomplished by the polynomial\n",
    "\n",
    "<p>\n",
    "$$\n",
    "P_k(a) = T_k\\left(\\frac{\\beta+\\alpha-2a}{\\beta-\\alpha}\\right)\\cdot T\\left(\\frac{\\beta+\\alpha}{\\beta-\\alpha}\\right)^{-1}\\,.\n",
    "$$\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def P(k, a, alpha=1, beta=10.0):\n",
    "    \"\"\"Rescaled Chebyshev polynomial.\"\"\"\n",
    "    assert beta > alpha\n",
    "    normalization = T(k, (beta+alpha)/(beta-alpha))\n",
    "    return T(k, (beta+alpha-2*a)/(beta-alpha))/normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "alpha, beta = 1.0, 10.0\n",
    "xs = np.linspace(0, beta, 100)\n",
    "plt.figure(figsize=(14, 8))\n",
    "plt.title('Rescaled chebyshev')\n",
    "plt.plot(xs, P(3, xs), 'g-', label='degree 3', **kwargs)\n",
    "plt.plot(xs, [P(3, alpha)]*len(xs), 'g--', label='max value')\n",
    "plt.plot(xs, P(6, xs), 'b-', label='degree 6', **kwargs)\n",
    "plt.plot(xs, [P(6, alpha)]*len(xs), 'b--', label='max value')\n",
    "_ = plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that doubling the degree has a much more dramatic effect on the magnitude of the polynomial in the interval $[\\alpha, \\beta].$\n",
    "\n",
    "Let's compare this beautiful Chebyshev polynomial side by side with the naive polynomial we saw earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 8))\n",
    "plt.title('Rescaled chebyshev vs naive polynomial')\n",
    "plt.plot(xs, P(6, xs), 'g-', label='deg-6 chebyshev', **kwargs)\n",
    "plt.plot(xs, [P(6, alpha)]*len(xs), 'g--', label='max value')\n",
    "plt.plot(xs, p(6, xs), 'b-', label='deg-6 naive', **kwargs)\n",
    "plt.plot(xs, [p(6, alpha)]*len(xs), 'b--', label='max value')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks promising. But what exactly is the error bound that comes out of it and how does it lead to an iterative algorithm? These are important questions that we'll answer next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"acceleration\"></a>\n",
    "\n",
    "## Accelerated gradient descent\n",
    "\n",
    "The Chebyshev polynomial leads to an accelerated version of gradient descent. Before we describe the iterative process, let's first see what error bound comes out of the Chebyshev polynomial.\n",
    "\n",
    "So, just how large is the polynomial in the interval $[\\alpha, \\beta]$? First, note that the maximum value is attained at $\\alpha$. Plugging this into the definition of the rescaled Chebyshev polynomial we get the upper bound for any $a\\in[\\alpha, \\beta],$\n",
    "\n",
    "<p>\n",
    "$$\n",
    "|P_k(a)| \\le P_k(\\alpha)= T\\left(\\frac{\\beta+\\alpha}{\\beta-\\alpha}\\right)^{-1}.\n",
    "$$\n",
    "</p>\n",
    "\n",
    "Recalling the condition number $\\kappa=\\beta/\\alpha,$ we have\n",
    "<p>\n",
    "$$\n",
    "\\frac{\\beta+\\alpha}{\\beta-\\alpha}\n",
    "=\\frac{\\kappa+1}{\\kappa-1}=1+\\epsilon\n",
    "$$\n",
    "for some $\\epsilon\\approx 2/\\kappa.$\n",
    "</p>\n",
    "\n",
    "We therefore only need to understand the value of $T_k$ at $1+\\epsilon.$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A well-known fact about the Chebyshev polynomials comes in handy.\n",
    "\n",
    "**Fact**. For $x>1,$ we have $T_k(x)=\\cosh(k\\cdot\\mathrm{arccosh}(x)).$\n",
    "\n",
    "Recall that $\\cosh(x) = (e^x+e^{-x})/2$ and $\\mathrm{arccosh}(x) = \\ln(x+\\sqrt{x^2-1}).$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, letting $\\phi=\\mathrm{arccosh}(1+\\epsilon),$\n",
    "<p>\n",
    "$$\n",
    "e^{\\phi} = 1+\\epsilon+\\sqrt{2\\epsilon+\\epsilon^2}\n",
    "\\ge 1+\\sqrt{\\epsilon}\\,.\n",
    "$$\n",
    "</p>\n",
    "\n",
    "Therefore,\n",
    "<p>\n",
    "$$\n",
    "T_k(1+\\epsilon)=\\frac{(e^{\\phi})^k+(e^{\\phi})^{-k}}{2}\n",
    "\\ge \\frac{\\big(1+\\sqrt{\\epsilon}\\big)^k}2\\,.\n",
    "$$\n",
    "</p>\n",
    "\n",
    "The reciprocal is what we needed to upper bound, so we have\n",
    "<p>\n",
    "$$\n",
    "T\\left(\\frac{\\beta+\\alpha}{\\beta-\\alpha}\\right)^{-1}\n",
    "\\le 2\\big(1+\\sqrt{\\epsilon}\\big)^{-k}\\,.\n",
    "$$\n",
    "</p>\n",
    "\n",
    "This establishes that the Chebyshev polynomial achieves the error bound\n",
    "<p>\n",
    "$$\n",
    "\\|e_k\\|\\le 2\\big(1+\\sqrt{\\epsilon}\\big)^{-k}\\|e_0\\|\n",
    "$$\n",
    "</p>\n",
    "\n",
    "Recalling that $\\epsilon\\approx 1/\\kappa,$ this means that for large $\\kappa,$ we get quadratic savings in the degree we need before the error drops of exponentially."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Chebyshev recurrence relation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the recursive definition of the Chebyshev polynomial, we directly get an iterative algorithm out of it. Transferring the recursive definition to our rescaled Chebyshev polynomial, we have\n",
    "\n",
    "<p>\n",
    "$$\n",
    "P_{k+1}(a) = (\\eta_k a + \\gamma_k)P_k(a) + \\mu_k P_{k-1}(a),\n",
    "$$\n",
    "</p>\n",
    "\n",
    "where we can work out the coefficients $\\eta_k,\\gamma_k,\\mu_k$ from the definition. Since $P_k(0)=1,$ we must have $\\gamma_k+\\mu_k=1.$ This leads to a simple update rule for our iterates:\n",
    "\n",
    "<p>\n",
    "$$\n",
    "\\begin{align}\n",
    "x_{k+1} &= (\\eta_k A + \\gamma_k)x_k + (1-\\gamma_k) x_{k-1}-\\eta_k b\\\\\n",
    "&= x_k - \\eta_k\\nabla f(x_k) + \\mu_k (x_k - x_{k-1})\n",
    "\\end{align}\n",
    "$$\n",
    "</p>\n",
    "\n",
    "We see that this update form is very similar to plain gradient descent except for the additional term $\\mu_k(x_k - x_{k-1}).$ This term can be interpreted as a *momentum* term, pushing the algorithm in the direction of where it was headed before.\n",
    "\n",
    "In the next lecture, we'll dig deaper into momentum and see how to generalize the result for quadratics to general convex functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## That's it. Thanks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
