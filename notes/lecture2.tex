\section{Lecture 2: Optimization}
\sectionlabel{optimization}


\begin{theorem}[Project Gradient Descent for Lipchitz Functions]
\theoremlabel{libpchitz}
Assume that function $f$ is convex, differntiable, and closed with bounded gradients. Let $L$ be the Lipchitz constant of $f$ over the convex domain $\Omega$. Let $R$ be the upper bound on the distance from the initial point $x_1$ to the optimal point $x^* = \arg\min_{x \in \Omega} f(x)$ (i.e. $\lVert x_1 - x^* \rVert_2$). Let $t$ be the number of iterations of project gradient descent.

If the learning rate $\eta$ is set to $\eta=\frac{R}{L \sqrt(t)}$, then $f\left(\frac{1}{t}\sum_{s=1}^t x_s\right) - f\left(x^*\right) \leq \frac{RL}{\sqrt{t}}$.

This means that the difference between the functional value of the average point during the optimization process from the optimal value is bounded above by a constant proportional to $\frac{1}{\sqrt{t}}$. 

\end{theorem}

Before proving the theorem, recall that
\begin{itemize}
    \item First order characterization of convexity: $f(y) = f(x) + \nabla f(x)^\top (y - x)$
    \item "Fundamental Theorem of Optimization": An inner product can be written as a sum of norms: $u^\top v = \frac{1}{2}(\lVert u \rVert^2 + \lVert v \lVert^2 - \lVert u - v \rVert^2$. This property can be seen by writing $\lVert u - v \rVert$ as $\lVert u - v \rVert = \lVert u \rVert^2 + \lVert v \lVert^2 - 2 u^\top v$.
    \item $L$-Lipchitz: For all $x$, $\lVert \nabla f(x) \rVert \leq L$.
    \item $\lVert \pi_\Omega (y) - x \rVert^2 \leq \lVert y - x \rVert^2 - \lVert y - \pi_\Omega (x) \rVert^2$ % From above TODO
\end{itemize}


\begin{proof}[Proof of \theoremref{libpchitz} for compact sets.]
The proof begins by first bounding the difference in function values $f(x_s) - f(x^*)$.
\begin{align}
    f(x_s) - f(x^*) \leq 
\end{align}
\end{proof}