\documentclass[12pt]{article}

\usepackage{macros}

\title{Problem Set 2 for EE227C (Spring 2018):\\
 Convex Optimization and Approximation }
\author{Instructor: Moritz Hardt\\
{\small Email: \tt hardt+ee227c@berkeley.edu}\\ ~\\
Graduate Instructor: Max Simchowitz\\
{\small Email: \tt msimchow+ee227c@berkeley.edu}\\ ~\\
}

\begin{document}
%\setenumerate[0]{leftmargin=0pt}
%\setenumerate[1]{leftmargin=50pt}
\setlist[enumerate,1]{leftmargin=0pt,label=\bf(\Alph*)}
\setlist[enumerate,2]{leftmargin=0pt,label=\bf(\Alph{enumi}.\arabic*)}


\maketitle

\section*{Problem 1: Backtracking Line Search}
	Let $f: \R^n \to \R$ be an $m$-strongly convex, $M$-smooth (and thus differentiable) function with global minimum $x^*$. Consider the following algorithm: 


	Initialize with an arbitrary $x_0 \in \R^n$, and fix parameters $\alpha \in (0,1/2),\beta \in (0,1)$. Then at each step $t = 1,2,\dots$, do the following: 
	%

	(a) Let $g_t = \nabla f(x_t)$. 
	%

	(b) For $k = \{0,1,\dots\}$ in sequence, check if the following ``sufficient decrease'' condition holds:
	\begin{eqnarray}
	f(x - tg_t) \le f(x) - \alpha \beta^k \cdot \|g_t\|^2
	\end{eqnarray}
	Assuming that this condition holds for some $k$ (you will show this), set $\eta_t = \beta^k$. 

	(c) Set $x_t \leftarrow x_{t-1}-\eta_tg_t$
	\begin{enumerate}
		\item Show that condition $1$ holds for all $t \in (0,1/M]$.
		\item Show that $\eta_t \ge \min\{1,\beta/M\}$.
		 %
		 Conclude that step (b) of the above algorithm aways terminates.
		\item Using part $b$, show that 
		\begin{eqnarray}
		f(x_t - \eta_t g_t) \le f(x) - \alpha \min\{1,\frac{\beta}{M}\}\|\nabla f(x_t)^2\|
		\end{eqnarray}
		\item Show that there is a constant $C = C(\alpha,\beta,M,m) < 1$ such 
		\begin{eqnarray}
		f(x_t - \eta_t g_t) - f(x) \le C(\alpha,\beta,M,m) \cdot (f(x_t) - f(x_t))
		\end{eqnarray}
	\end{enumerate}


\section*{Problem 2: Random Descent Directions}

	Let $f: \R^n \to \R$ be an $m$-strongly convex, $M$-smooth (and thus differentiable) function with global minimum $x^*$. 
%
	Consider the following algorithm: Initialize with an arbitrary $x_0 \in \R^n$. 
%
	Then at each step $t = 1,2,\dots$, do the following: 
	%

	(a) Choose $g_t \unifsim \spheren$ (equivalently, $g_t$ has the distribution of $\frac{g}{\|g\|}$, where $g\sim \mathcal{N}(0,I_n)$). 
	%

	(b) Compute a step size $\eta_t := \min_{\eta \ge 0} f(x_{t-1}-\eta g_t)$.
	%

	(c) set $x_t \leftarrow x_{t-1}-\eta g_t$

\begin{enumerate}
	\item Prove that the above algorithm is a (non-strict) descent method; that is $f(x_t)$ is non-increasing in $t$. Also prove that unless $x_t = x_*$, $f(x_{t+1}) < f(x_t)$ with probability $1/2$. 
	\item Prove that there exists a numerical constant $C$ such that, if 
	\begin{eqnarray}
	t \ge T(\epsilon) := C n \cdot \frac{M}{m} \log (\frac{f(x_0) - f(x^*)}{\epsilon})~,
	\end{eqnarray}
	then $\Exp[f(x_t) - f(x^*)] \le \epsilon$. 
	\item Ammend the stated algorithm to use line search instead of solving for the exactly-optimal step size. Are the rates qualitatively similar?
\end{enumerate}
\section*{Problem 3: Sh*t about Quadratics}
	In this problem, you are going to test the sharpness of our upper and lower bounds for quadratics on a randomly generated instance. Let $\cS = \{1,.5,.2,.1,.05\}$ and $n = 500$. Now, for each $\epsilon \in \cS$, generate the random matrix $\mathbf{M}$ as follows: 

	(a)Generate an $n \times n$ random wigner matrix $\mathbf{W} \in \R^{n \times n}$, 
		\begin{eqnarray*}
		\mathbf{W} = \frac{1}{\sqrt{2n}} (\mathbf{X} + \mathbf{X}^{\top})
		\end{eqnarray*}
		where $\mathbf{X} \in \R^{n \times n}$ is a matrix with i.i.d standard normal entries. 
	
	(b) Generate $\mathbf{u}$ uniformly on the unit sphere, and set $\mathbf{M} = \mathbf{W} + (1 + \epsilon)\mathbf{u}\mathbf{u}^{\top}$. 

	(c) Now, for each $\epsilon \in \cS$, do the following:
	\begin{enumerate}
		\item Conduct trials $t = 1,2,\dots,10$.
		\begin{enumerate}
		\item Generate $\mathbf{M}$ as above, and a random vector $\mathbf{v}$ uniformly on the unit sphere. 
		\item Set $\gamma = 2\lambda_{\max}(\mathbf{W}) - \lambda_{2}(\mathbf{W})$, and define the matrix $\mathbf{N} = \gamma I  - \mathbf{M}$. Definally, define the function $\mathbf{f}(x) = \min_{x} x^T \mathbf{N} x - 2 \langle \mathbf{v}, x \rangle$. 
		\item Setting $x_0 = 0$, run gradient descent, a heavy-ball method or nesterov method to solve $\min_{x} \mathbf{f}(x)$ for a good number of iterations (use your discretion). You may compute the eigenvalues of $\mathbf{N}$ to tune your step parameters. 
		\item For both gradient descent and heavy-ball, record for each trial iteration $s$, the difference between $\mathbf{f}(x_s) - \min_{x} \mathbf{f}(x)$ for each iteration. 
		\item Using the step sizes, largest/smallest eigenvalues of $\mathbf{N}$, and the initial point $x_0 = 0$, compute a worst case upper bound for $\mathbf{f}(x_s) - \min_{x} \mathbf{f}(x)$ for each iteration $s$ of gradient descent and the heavy ball method.
		\item Run gradient descent, but this time compute the optimality gap unising ``best'' iterate in the Krylov space. THat is, compute 
		\begin{eqnarray}
		\min_{x \in \mathrm{span}(x_1,\dots,x_s)}\mathbf{f}(x) - \min_{x} \mathbf{f}(x)
		\end{eqnarray}
		\item After each trial, you should have a list of 5 values for each iterate $s$: an upper bound for gradient descent, the rate actually attained by gradient descent, an upper bound for heavy ball/nesterov, the rate actualy attained by heavy ball/nesterov, and the ``optimal'' krylov algorith,
	\end{enumerate}
	\item For each of the lists above, average all $10$ trials and plot them on the same plot. How sharp are the upper bounds?
	\end{enumerate}




\end{document}
