\documentclass[12pt]{article}

\usepackage{macros}

\title{Problem Set 1 for EE227C (Spring 2018):\\
 Convex Optimization and Approximation }
\author{Instructor: Moritz Hardt\\
{\small Email: \tt hardt+ee227c@berkeley.edu}\\ ~\\
Graduate Instructor: Max Simchowitz\\
{\small Email: \tt msimchow+ee227c@berkeley.edu}\\ ~\\
}

\begin{document}

\maketitle




\section*{Problem 1: Existence of the Subgradients}
Let $\cX$ be a convex set. Prove that that given any convex function $f: \cX \to \R$ and any $x \in \cX$, there exists at least one vector $g$ - called a \emph{subgradient} of $f$ at $x$ - such that $f(y) \ge f(x) + \langle g, y - x \rangle$ for all $y \in \cX$. You will do so following the steps below. 
\paragraph{i)} Define the \emph{Epigraph} of $f$, $\Epi(f) := \{(x,t) \in \cX \times \R: f(x) \le t\}$. Prove the $\Epi(f)$ is convex. 

\paragraph{ii)} Recall the following definitions from real analysis
\begin{definition*}[Boundary and Interior] For a set $\cX \subset \R^d$, its closure $\overline{\cX}$ is defined as the set of all $x \in \R^d$ (not necessarily in $\cX$) such that, for all $\epsilon > 0$, there exists a $y \in \cX$ such that $\|x - y\| \le \epsilon$. In other words, for every $\epsilon > 0$, the ball of radius $\epsilon$ around $x$ intersects $\cX$. $\Int(\cX)$ is defined as the set of all points $x \in \cX$ such that there exists an $\epsilon > 0$ for which, for all $y: \|x-y\| \le \epsilon$, $y \in \cX$; it other words, for some $\epsilon > 0$, the ball of radius $\epsilon > 0$ around $x$ lies entirely in $\cX$. Lastly, we define the boundary $\Bd(\cX) := \overline{\cX} - \Int(\cX) = \{x \in \overline{\cX} : x \notin \Int(\cX)\}$. 
\end{definition*}


Using the separating hyperplane theorem from the notes (the full version, which applies to arbitrary convex sets not just compact ones), prove the supporting hyperplane theorem.
\begin{theorem*}[Supporting Hyperplane] Let $\cC \subset \R^d$ be a convex set, and let $x \in \Bd(\cC)$. Then, there exists a $w \in \R^d$ such that, for all $y \in \cC$, $\langle w, y - x \rangle \ge 0$. 
\end{theorem*}
\emph{Hint:  Find two (not-necessarily compact!) convex sets to apply the separating hyperplane theorem. You might want $\Int(\cC)$ to be one of them - and you should check that $\Int(\cC)$ is convex } 


\paragraph{iii)} Using part $i)$ and $ii)$, prove the existence of a subgradient. 

\paragraph{iv)} Let $\{g_{\alpha}\}$ be a (possibly infinite, uncountable) family of convex functions, and suppose that $g_{\alpha}(x) < \infty$ for all $x \in \cX$. Show that $g(x) := \sup_{\alpha} g_{\alpha}(x)$ is convex on $\cX$ (you may assume $g(x)$ is finite).

\paragraph{v)} Using what we've proven about subgradients, prove that a function $g: \cX \to \R$ is convex if and only if it can be written as the supremum of affine functions (e.g. supremum of functions of the form $g_{\alpha}(x) = \langle a_{\alpha}, x \rangle + b_{\alpha}$)

\section*{Problem 2: Properties of Subgradients}

\subparagraph{i)} Show by way of example that the subgradient is not necssarily unique, but that the set of all subgradients is closed and convex. We will denote this \emph{set} $\partial f(x)$.

\subparagraph{ii)} Show that a convex $f$ is differentiable at $x$ if and only if $\partial f(x) = \{\nabla f(x)\}$. 

\subparagraph{iii)} Show that if $g_1 \in \partial f_1(x)$ and $g_2 \in \partial f_2(x)$, then $g_1 + g_2 \in \partial(f_1 + f_2)(x)$. 



\subparagraph{vi)} Let $f(x) = \sup_{\alpha} g_{\alpha}(x)$ which $g_{\alpha}$ convex. Show that $\Conv \{\partial g_{\alpha}(x) |  g_{\alpha}(x) =f(x) \} \subseteq \partial f$. 

\subparagraph{(v)} Prove that if $g_{\alpha}(x) = w_{\alpha}^\top x + b_{\alpha}$ are a compact family of affine functions (i.e. $\{(w_{\alpha},b_{\alpha})\} \subset \R^{d+1}$ is compact), then the converse is true, namely $\partial f \subset \Conv \{\partial g_{\alpha}(x) |  g_{\alpha}(x) =f(x) \} $. 


\section*{Problem 3: Subgradients of Norms}
\paragraph{A)} Subgradient of the $L_1$ and $L_{\infty}$-norms
\subparagraph{i))} Prove that, for all $x \in \R^d$, $\|x\|_1 = \sup_{y: \|y\|_{\infty} \le 1}\langle x, y \rangle$, $\|x\|_{\infty} = \sup_{y:\|y\|_1 \le 1}\langle x, y \rangle$. 
\subparagraph{ii))} Compute $\partial \|x\|_1$ and $\partial \|x\|_{\infty}$
\paragraph{A)} Subgradient of the $L_1$-norm
\subparagraph{i))} Let $A \in \R^{m \times n}$. Let $\sigma_i(\cdot)$ denote the $i$-th singular value of a matrix. Using the inequality $\sum_{i=1}^{\min(n,m)}\sigma_i(AB) \le \sum_{i=1}^{\min(n,m)} \sigma_i(A)\sigma_i(B)$ for all $A \in \R^{m \times n}$ and $B \in \R^{n \times m}$, prove the follow: For all $X \in \R^{m \times n}$, 
\begin{eqnarray}
\|X\|_{\op} :=  \max_{Y \in \R^{m \times n}  \|Y\|_{\nuc} \le 1} \langle X, Y \rangle  \text{ and } \|X\|_{\nuc} =  \max_{Y \in \R^{m \times n} :\|Y\|_{\op} \le 1} \langle X, Y \rangle ~,
\end{eqnarray}
where $\|X\|_{\op} := \sigma_{\max}(X)$, $ \|Y\|_{\nuc} := \sum_{i=1}^{\min(n,m)} \sigma_i(Y)$, and $\langle X, Y \rangle := \tr(X^\top Y)$. You may want to refresh yourself on the relationship between traces, eigenvalues and singular values. 
\subparagraph{ii))} Compute $\partial \|X\|_{\op}$ and $\partial \|X\|_{\nuc}$. Under what conditions is each subgradient unique?
\paragraph{C)} Let  $\|\cdot\|$ be an arbitary norm (not necessarily Euclidean!) on $\R^d$. Define the dual norm $\|y\|_* := \sup_{x: \|x\| \le 1}\langle x,y \rangle$. 
\subparagraph{i)} Show that the dual norm is a norm, and describe its subgradient.
\subparagraph{ii)} Let $f$ be a convex function on a convex domain $\cX$. Show that $f$ is $L$-Lipschitz on $\cX$ if an only if, for all $x \in \cX$ and all $g \in \partial f(x)$, $\|g\|_{*} \le L$. 


\section*{Problem 4: Extensions for Gradient Descent}
\paragraph{A)} Generalizations of SGD
\subparagraph{i))} Prove the following statement:
\begin{proposition*} Let $\Omega$ be a convex domain of radius $R$, and let $f$ be a convex function on $\Omega$. Let $x_0 \in \Omega$, and let $x_{t} = \Pi_{\Omega}(x_{t-1} - \eta g_t )$, where $\Exp[g_t | g_1,\dots,g_{t-1}] \in \partial f(x_t)$, and $\sup_{t} \Exp[\|g_t\|^2] \le L$ and $\eta = \frac{LR}{\sqrt{T}}$.  Prove that 
\begin{eqnarray}
 f(\frac{1}{T}\sum_{t=1}^T x_t) \le \inf_{x \in \Omega} f(x) + \dots
\end{eqnarray}
You fill in the $\dots$.
\end{proposition*}
\subparagraph{ii)} Prove the following statement:
\begin{proposition*}  Let $\Omega$ be a convex domain of radius $R$, Let $f_1,f_2,\dots,f_T$ be $L$-Lipschitz, convex functions on $\Omega$. Given any $x_0 \in \Omega$, let $x_{t} = \Pi_{\Omega}(x_{t-1} - \eta g_t ))$, where $g_t \in \partial f_t(x_t)$, and  $\eta = \frac{LR}{\sqrt{T}}$. Prove that
\begin{eqnarray}
\frac{1}{T}\sum_{t=1}^{T} f(\frac{1}{T}\sum_{t=1}^T x_t) \le \frac{1}{T}\sum_{t=1}^Tf_t(x_t) \le \inf_{x \in \Omega}\frac{1}{T}\sum_{t=1}^Tf_t(x_t) + \frac{\dots}{}
\end{eqnarray}
You fill in the $\dots$.
\end{proposition*}

\paragraph{B)}In this problem we show that in the stochastic setting, smoothness of the function $f$ does not help. Let $\Omega = [-1,1]$, let $\sigma$ be a random variable with $\Pr[\sigma = 1] = \Pr[\sigma = -1] = 1/2$, fix an $\epsilon \in (0,1/4)$. Let $z_1,z_2,\dots,z_T$ be $T$ i.i.d random variables, such that $z_i | \sigma$ are mutually independent, and 
\begin{eqnarray}
\Pr[z_i =  1 | \sigma] = 1/2 + \sigma \epsilon \text{ and } \Pr[z_i =  -1 | \sigma] = 1/2 - \sigma \epsilon 
\end{eqnarray}
You will need the following information
\begin{lemma*} Let $\sigma$ and $z_1,z_2,\dots,z_T$ be as above. Then there exists a universal constant $C$ such that, if $T \le C \epsilon^2$, any algorithm which returns an estimate $\widehat{\sigma}$ of $\sigma$ from observing $z_1,z_2,\dots,z_T$ satisfies $\Pr[\widehat{\sigma} \ne \sigma] \ge \frac{1}{4}$, where $\Pr$ is taking over the randomness in $\sigma$, $z_1,\dots,Z_T$, and any randomness in the algorithm. 
\end{lemma*}
\subparagraph{i)} Construct a function on $f_{\sigma}$ such that $\Exp[z_i | \sigma] = \nabla f_{\sigma}(x)$ for all $x \in \Omega$. What is the optimum $x^*_{\sigma} $of $f_{\sigma}$? What is the ``smoothness'' of $f_{\sigma}$?
\subparagraph{ii)} Show that there exists a universal constant $c$ such that the following hold: Fix a $T \in \mathbb{N}$, and let $\cA$ be an algorithm which is allowed to make $T$ queries $x_t \in [-1,1]$ and $g_t$, where $\cA$ decides $x_t$, and recieves responses $g_t$ such that $\Exp[g_t] = \nabla f(x_t)$ and $|g_t| \le 1$ as.s. Then, there is a $0$-smooth, $1$-Lipschitz function $f$ and a mechanism for generating noisy subgradients $g_t$ such, for any algorithm using $g_t$ as gradient queries, the iterate $x_{T+1}$ satisfies
\begin{eqnarray}
\Exp[f(x_{T+1})] - \inf_{x \in [-1,1]} f(x)\ge c /\sqrt{T}
\end{eqnarray}
\section*{Problem 5: Generalized Projections}
In this problem, we introduce a useful generalization of gradient descent. Let $\cX \subseteq \cD \subseteq \R^d$ be convex sets, and let $\Phi: \cD \to \R$ be a strictly convex, continuously differentiable map such that $\|\nabla \Phi(x)\|$ diverges on $\Bd(\cD)$, and $\nabla \Phi(\cD) = \R^d$. We call $\Phi$ a \emph{mirror map}.

\paragraph{A)} Define the \emph{Bregman Divergence}
\begin{eqnarray}
D_{\Phi}(x,y) = f(x) - f(y) - \nabla f(y)^{\top}(x-y)
\end{eqnarray}
and the associated $\Phi$ projection
\begin{eqnarray}
\Pi_{\cX}^{\Phi}(y):= \arg\min_{x \in \cX} D_{\Phi(x,y)}
\end{eqnarray}
Show that $\Phi(x) = \frac{1}{2}\|x\|^2_2$ is a mirror map, and compute $D_{\Phi}(x,y)$ and explain what $\Pi_{\cX}^{\Phi}(y)$ corresponds to
\paragraph{B)} Prove that, for all $x \in \cX$ and $y \in \cD$,
\begin{eqnarray}
(\nabla \Phi(\Pi_{\cX}^{\Phi}(y)) - \nabla \Phi(y))^\top(\Pi_{\cX}^{\Phi}(y) - x) \le 0
\end{eqnarray}
and conclude that
\begin{eqnarray}
D_{\phi}(x,\Phi_{x}(y)) + D_{\phi}(\Phi_{x}(y),y) \le D_{\Phi}(x,y) 
\end{eqnarray}
What does this reduce to when $\Phi(x) = \frac{1}{2}\|x\|^2_2$?
\paragraph{C)} Consider the following algorithm, known as mirror descent. Let $\cX \subset \cD$ and $\Phi$ be as above, let $f: \cX \to \R$ be convex, let $x_1  \in \cX$. Fix an $\eta > 0$. For $t \ge 1$, define $y_{t+1}$ such that $\nabla \Phi(y_{t+1}) - \nabla \Phi(x_{t}) = \eta g_t$, where $g_t \in \partial f(x_t)$. Prove the following:
\begin{theorem*} Let $\|\cdot\|$ be an \emph{arbitrary} norm on $\cX$, and suppose that $\Phi$ is a $\kappa$ strongly-convex mirror map with respect to $\|\cdot\|$ on $\cX$. Suppose that $f$ is L-Lipschitz with respect to $\|\cdot\|$. Prove that 
\begin{eqnarray}
f(\sum_{s=1}^T x_s) - \min_{x \in \cX} f(x) \le \frac{D(x,\pi)}{\eta} + \eta \frac{L^2 T}{\kappa}
\end{eqnarray}
\end{theorem*}
Recall that $\Phi$ is $\kappa$-strongly convex with respect to $\|\cdot\|$ if and only $\Phi(x)  - \Phi(y) \le \nabla \Phi(x)^\top (x-y) + \frac{\kappa}{2}\|x-y\|^2$. 
\paragraph{D)} A common setup for mirror descent is on the simplex, where $\cD: \{x: x_i > 0 \forall i \in [d]\}$, and $\cX := \{x \in \cD:\|x\|_1 = 1\}$. Given an iterate $x_t$, compute the updates $y_{t+1}$ and $x_{t+1}$. 

\end{document}
